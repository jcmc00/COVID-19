{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults to 32 in augmentations (cifar) set to imagenet\n",
    "augmentations.IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughts - this samples and augments the same image multiple times (up to 4)\n",
    "# Then Applies a convex comination of these augments to increase robustness/generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams and args here - can later class or modularise\n",
    "bs = 8\n",
    "epochs = 15\n",
    "pretrained = True \n",
    "lr = 0.1 # initial learning rate - schedulers will anneal this\n",
    "momentum = 0.9 # as standard. Uses 0.95 - 0.85 in OneCycle learners\n",
    "wd = 0.001\n",
    "\n",
    "mixture_depth = 0 # setting to 0 uses np.rand.int(1, 4) for operations per image (I think?)\n",
    "mixture_width = 3 # number of augmentation chains to mix per augmented example (3 default)\n",
    "model = 'resnet50'\n",
    "\n",
    "resume = False # normally a path - but passes a bool check first\n",
    "save_dir = False # required currently train - cannot be false\n",
    "\n",
    "all_ops = True # use all augmentation operations for training\n",
    "aug_severity = 1 # severity of base augmentation operators\n",
    "aug_prob_coeff = 1. # probability distribution coefficients\n",
    "no_jsd = False # use JSD loss\n",
    "\n",
    "clean_path = Path('/home/jack/Documents/DL/Experiments/covid-19/dataset')\n",
    "evaluate = True # calculates val loss (and normally corruption loss) if True\n",
    "\n",
    "evaluate_corrupt = False # evaluate corruption loss (requires a corrupted dataset)\n",
    "corrupt_path = None # path for corrupted dataset\n",
    "num_workers = 4\n",
    "print_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTIONS = [\n",
    "    'gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n",
    "    'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n",
    "    'brightness', 'contrast', 'elastic_transform', 'pixelate',\n",
    "    'jpeg_compression'\n",
    "]\n",
    "\n",
    "# Raw AlexNet errors taken from https://github.com/hendrycks/robustness\n",
    "ALEXNET_ERR = [\n",
    "    0.886428, 0.894468, 0.922640, 0.819880, 0.826268, 0.785948, 0.798360,\n",
    "    0.866816, 0.826572, 0.819324, 0.564592, 0.853204, 0.646056, 0.717840,\n",
    "    0.606500\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can break main out - but optimizer still need to model.parameters() somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugMixDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Dataset wrapper to perform AugMix augmentation.\"\"\"\n",
    "\n",
    "  def __init__(self, dataset, preprocess, no_jsd=False):\n",
    "    self.dataset = dataset\n",
    "    self.preprocess = preprocess\n",
    "    self.no_jsd = no_jsd\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    x, y = self.dataset[i]\n",
    "    if self.no_jsd:\n",
    "      return aug(x, self.preprocess), y\n",
    "    else:\n",
    "      im_tuple = (self.preprocess(x), aug(x, self.preprocess), aug(x, self.preprocess))\n",
    "      return im_tuple, y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does cosine annealing after an epoch (built in to many default optimizers/schedulers)\n",
    "# OneCycle Policy & CyclicLearners both do this for example\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "  \"\"\"Sets the learning rate to the initial LR (linearly scaled to batch size) decayed by 10 every n / 3 epochs.\"\"\"\n",
    "  b = bs / 256.\n",
    "  k = epochs // 3\n",
    "  if epoch < k:\n",
    "    m = 1\n",
    "  elif epoch < 2 * k:\n",
    "    m = 0.1\n",
    "  else:\n",
    "    m = 0.01\n",
    "  lr = lr * m * b\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "  \"\"\"Computes the accuracy over the k top predictions for the specified values of k.\"\"\"\n",
    "  with torch.no_grad():\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "      correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "      res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mce(corruption_accs):\n",
    "  \"\"\"Compute mCE (mean Corruption Error) normalized by AlexNet performance.\"\"\"\n",
    "  mce = 0.\n",
    "  for i in range(len(CORRUPTIONS)):\n",
    "    avg_err = 1 - np.mean(corruption_accs[CORRUPTIONS[i]])\n",
    "    ce = 100 * avg_err / ALEXNET_ERR[i]\n",
    "    mce += ce / 15\n",
    "  return mce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(image, preprocess):\n",
    "  \"\"\"Perform AugMix augmentations and compute mixture.\n",
    "  Args:\n",
    "    image: PIL.Image input image\n",
    "    preprocess: Preprocessing function which should return a torch tensor.\n",
    "  Returns:\n",
    "    mixed: Augmented and mixed image.\n",
    "  \"\"\"\n",
    "  aug_list = augmentations.augmentations\n",
    "  if all_ops:\n",
    "    aug_list = augmentations.augmentations_all\n",
    "\n",
    "  ws = np.float32(\n",
    "      np.random.dirichlet([aug_prob_coeff] * mixture_width))\n",
    "  m = np.float32(np.random.beta(aug_prob_coeff, aug_prob_coeff))\n",
    "\n",
    "  mix = torch.zeros_like(preprocess(image))\n",
    "  for i in range(mixture_width):\n",
    "    image_aug = image.copy()\n",
    "    depth = mixture_depth if mixture_depth > 0 else np.random.randint(\n",
    "        1, 4)\n",
    "    for _ in range(depth):\n",
    "      op = np.random.choice(aug_list)\n",
    "      image_aug = op(image_aug, aug_severity)\n",
    "    # Preprocessing commutes since all coefficients are convex\n",
    "    mix += ws[i] * preprocess(image_aug)\n",
    "\n",
    "  mixed = (1 - m) * preprocess(image) + m * mix\n",
    "  return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test is standard cross entropy loss between clean images x_val and y_val\n",
    "def test(net, test_loader):\n",
    "  \"\"\"Evaluate network on given dataset.\"\"\"\n",
    "  net.eval()\n",
    "  total_loss = 0.\n",
    "  total_correct = 0\n",
    "  with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "      images, targets = images.cuda(), targets.cuda()\n",
    "      logits = net(images)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "      pred = logits.data.max(1)[1]\n",
    "      total_loss += float(loss.data)\n",
    "      total_correct += pred.eq(targets.data).sum().item()\n",
    "\n",
    "  return total_loss / len(test_loader.dataset), total_correct / len(\n",
    "      test_loader.dataset)\n",
    "\n",
    "\n",
    "def test_c(net, test_transform):\n",
    "  \"\"\"Evaluate network on given corrupted dataset.\"\"\"\n",
    "  corruption_accs = {}\n",
    "  for c in CORRUPTIONS:\n",
    "    print(c)\n",
    "    for s in range(1, 6):\n",
    "      valdir = os.path.join(corrupted_data, c, str(s))\n",
    "      val_loader = torch.utils.data.DataLoader(\n",
    "          datasets.ImageFolder(valdir, test_transform),\n",
    "          batch_size=eval_batch_size,\n",
    "          shuffle=False,\n",
    "          num_workers=num_workers,\n",
    "          pin_memory=True)\n",
    "\n",
    "      loss, acc1 = test(net, val_loader)\n",
    "      if c in corruption_accs:\n",
    "        corruption_accs[c].append(acc1)\n",
    "      else:\n",
    "        corruption_accs[c] = [acc1]\n",
    "\n",
    "      print('\\ts={}: Test Loss {:.3f} | Test Acc1 {:.3f}'.format(\n",
    "          s, loss, 100. * acc1))\n",
    "\n",
    "  return corruption_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer):\n",
    "  \"\"\"Train for one epoch.\"\"\"\n",
    "  net.train()\n",
    "  data_ewma = 0.\n",
    "  batch_ewma = 0.\n",
    "  loss_ewma = 0.\n",
    "  acc1_ewma = 0.\n",
    "  acc5_ewma = 0.\n",
    "\n",
    "  end = time.time()\n",
    "  for i, (images, targets) in enumerate(train_loader):\n",
    "    # Compute data loading time\n",
    "    data_time = time.time() - end\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if no_jsd:\n",
    "      images = images.cuda()\n",
    "      targets = targets.cuda()\n",
    "      logits = net(images)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "      acc1, acc5 = accuracy(logits, targets, topk=(1, 5))  # pylint: disable=unbalanced-tuple-unpacking\n",
    "    else:\n",
    "      images_all = torch.cat(images, 0).cuda()\n",
    "      targets = targets.cuda()\n",
    "      logits_all = net(images_all)\n",
    "      logits_clean, logits_aug1, logits_aug2 = torch.split(logits_all, images[0].size(0))\n",
    "\n",
    "      # Cross-entropy is only computed on clean images\n",
    "      loss = F.cross_entropy(logits_clean, targets)\n",
    "\n",
    "      p_clean, p_aug1, p_aug2 = F.softmax(logits_clean, dim=1), \\\n",
    "                                F.softmax(logits_aug1, dim=1), \\\n",
    "                                F.softmax(logits_aug2, dim=1)\n",
    "\n",
    "      # Clamp mixture distribution to avoid exploding KL divergence\n",
    "      p_mixture = torch.clamp((p_clean + p_aug1 + p_aug2) / 3., 1e-7, 1).log()\n",
    "      loss += 12 * (F.kl_div(p_mixture, p_clean, reduction='batchmean') +\n",
    "                    F.kl_div(p_mixture, p_aug1, reduction='batchmean') +\n",
    "                    F.kl_div(p_mixture, p_aug2, reduction='batchmean')) / 3.\n",
    "      acc1, acc5 = accuracy(logits_clean, targets, topk=(1, 5))  # pylint: disable=unbalanced-tuple-unpacking\n",
    "\n",
    "    loss.backward()  \n",
    "    optimizer.step() \n",
    "\n",
    "    # Compute batch computation time and update moving averages.\n",
    "    batch_time = time.time() - end\n",
    "    end = time.time()\n",
    "\n",
    "    data_ewma = data_ewma * 0.1 + float(data_time) * 0.9\n",
    "    batch_ewma = batch_ewma * 0.1 + float(batch_time) * 0.9\n",
    "    loss_ewma = loss_ewma * 0.1 + float(loss) * 0.9\n",
    "    acc1_ewma = acc1_ewma * 0.1 + float(acc1) * 0.9\n",
    "    acc5_ewma = acc5_ewma * 0.1 + float(acc5) * 0.9\n",
    "\n",
    "    if i % print_freq == 0:\n",
    "      print(\n",
    "          'Batch {}/{}: Data Time {:.3f} | Batch Time {:.3f} | Train Loss {:.3f} | Train Acc1 '\n",
    "          '{:.3f} | Train Acc5 {:.3f}'.format(i, len(train_loader), data_ewma,\n",
    "                                              batch_ewma, loss_ewma, acc1_ewma,\n",
    "                                              acc5_ewma))\n",
    "      print('\\n') \n",
    "\n",
    "  return loss_ewma, acc1_ewma, batch_ewma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "  # Load datasets\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "      [transforms.RandomResizedCrop(224),\n",
    "       transforms.RandomHorizontalFlip()])\n",
    "    \n",
    "    preprocess = transforms.Compose(\n",
    "      [transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)])\n",
    "    test_transform = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      preprocess,\n",
    "  ])\n",
    "\n",
    "    traindir = os.path.join(clean_path, 'train')\n",
    "\n",
    "    valdir = os.path.join(clean_path, 'val')\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "    train_dataset = AugMixDataset(train_dataset, preprocess)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                              train_dataset,\n",
    "                              batch_size = bs,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                              datasets.ImageFolder(valdir, test_transform),\n",
    "                              batch_size = bs,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(model))\n",
    "        net = models.__dict__[model](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(model))\n",
    "        net = models.__dict__[model]()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "      net.parameters(),\n",
    "      lr = lr,\n",
    "      momentum = momentum,\n",
    "      weight_decay = wd)\n",
    "    \n",
    "  # Distribute model across all visible GPUs\n",
    "    net = torch.nn.DataParallel(net).cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            best_acc1 = checkpoint['best_acc1']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print('Model restored from epoch:', start_epoch)\n",
    "\n",
    "    if evaluate:\n",
    "        test_loss, test_acc1 = test(net, val_loader)\n",
    "        print('Clean\\n\\tVal Loss {:.3f} | Val Acc1 {:.3f}'.format(test_loss, 100 * test_acc1))\n",
    "\n",
    "    if evaluate_corrupt:\n",
    "        corruption_accs = test_c(net, test_transform)\n",
    "        for c in CORRUPTIONS:\n",
    "            print('\\t'.join([c] + map(str, corruption_accs[c])))\n",
    "\n",
    "        print('mCE (normalized by AlexNet): ', compute_mce(corruption_accs))\n",
    "#         return\n",
    "\n",
    "    if save_dir: # making it so only saves logs if you provide a savedir (can False it)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            raise Exception('%s is not a dir' % save_dir)\n",
    "\n",
    "        log_path = os.path.join(save_dir, 'imagenet_{}_training_log.csv'.format(model))\n",
    "        with open(log_path, 'w') as f:\n",
    "            f.write('epoch,batch_time,train_loss,train_acc1(%),test_loss,test_acc1(%)\\n')\n",
    "\n",
    "    best_acc1 = 0\n",
    "    print('Beginning training from epoch:', start_epoch + 1)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f'epoch : {epoch}')\n",
    "#         adjust_learning_rate(optimizer, epoch) # this is similar to a scheduler step (potentially replace?)\n",
    "\n",
    "        train_loss_ewma, train_acc1_ewma, batch_ewma = train(net, train_loader, optimizer)\n",
    "        test_loss, test_acc1 = test(net, val_loader)\n",
    "\n",
    "        is_best = test_acc1 > best_acc1\n",
    "        best_acc1 = max(test_acc1, best_acc1)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "        if save_dir:\n",
    "            save_dir_path = os.path.join(save_dir, 'checkpoint.pth.tar')\n",
    "            torch.save_dir(checkpoint, save_dir_path)\n",
    "            if is_best:\n",
    "                shutil.copyfile(save_dir_path, os.path.join(save_dir, 'model_best.pth.tar'))\n",
    "\n",
    "            with open(log_path, 'a') as f:\n",
    "                  f.write('%03d,%0.3f,%0.6f,%0.2f,%0.5f,%0.2f\\n' % (\n",
    "                  (epoch + 1),\n",
    "                  batch_ewma,\n",
    "                  train_loss_ewma,\n",
    "                  100. * train_acc1_ewma,\n",
    "                  test_loss,\n",
    "                  100. * test_acc1,\n",
    "              ))\n",
    "\n",
    "        print(\n",
    "            'Epoch {:3d} | Train Loss {:.4f} | Test Loss {:.3f} | Test Acc1 '\n",
    "            '{:.2f}'\n",
    "            .format((epoch + 1), train_loss_ewma, test_loss, 100. * test_acc1))\n",
    "\n",
    "        if evaluate_corrupt:\n",
    "            corruption_accs = test_c(net, test_transform)\n",
    "            for c in CORRUPTIONS:\n",
    "                print('\\t'.join(map(str, [c] + corruption_accs[c])))\n",
    "\n",
    "            print('mCE (normalized by AlexNet):', compute_mce(corruption_accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
